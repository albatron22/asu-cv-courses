import marimo

__generated_with = "0.16.0"
app = marimo.App(width="medium")


@app.cell
def _():
    import marimo as mo
    return (mo,)


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    # Л8: Трекинг движущихся объектов

    **CV (Computer Vision)**, или *машинное зрение*, — это область искусственного интеллекта, которая занимается разработкой алгоритмов и методов, позволяющих компьютерам "видеть" и интерпретировать изображения и видео. Основная задача компьютерного зрения — автоматическое извлечение полезной информации из визуальных данных, например, распознавание объектов, определение их местоположения или анализ сцен. Это может включать такие приложения, как автоматизация вождения, распознавание лиц и обработка медицинских изображений.

    **OpenCV (Open Source Computer Vision Library)** — библиотека алгоритмов компьютерного зрения, обработки изображений и численных алгоритмов общего назначения с открытым кодом

    **Отслеживание движущихся объектов** – это ключевой этап в обработке компьютерного зрения, позволяющий системам следить за объектами в движении через последовательные кадры видеопотока. Этот процесс важен для непрерывного мониторинга объектов в реальном времени и выявления их перемещений и изменений в динамике сцены.
    """
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ## ☝️ПОРЯДОК ВЫПОЛНЕНИЯ

    ЛР выполняется последователньно. По мере продвижения по работе вы будете получать задания для закрепления прочитанной теории

    ✏️ **ЗАДАНИЕ 1.**
    ✏️ **ЗАДАНИЕ 2.** и т.д.

    Для каждого задания создается отдельный скрипт Python. Всего заданий будет 4. В качестве отчета необходимо предоставить файлы кода для каждого задания. Отчет закреплять на moodle.
    """
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ## 1/ Подготовка среды выполнения
    На данной этапе вам наобходимо подготовить виртуальное окружение и установить все необходимые библиотеки.

    1. Создать и активировать (или только активировать, если ранне создавали) виртуальной окружение `python`.

    Создайте на диске свою раюочую папку. Откройте терминал и перейдите в свою рабочую директорию
    ```
    cd path/to/your/workspace
    ```
    Далее создаем виртуальное окружение с помощью `python-venv`
    ```
    python -m venv env
    ```
    *активируем виртуальное окружение*
    для CMD:
    ```
    env\Scripts\activate
    ```
    для PowerShell
    ```
    env\Scripts\Activate.ps1
    ```
    для bash
    ```
    source env/bin/activate
    ```
    **Примечание.** `env` - это название вашего виртуального окружения, назвать его можете как угодно.

    После этого можем выбрать наш локальный интерпрететор pyhton, нажав на кнопку выше "Select kernel".

    1. Устанавливаем все необходимые библиотеки

    **Примечание.** Библиотеки установятся внутрь вашего виртуального окружения.

    Нам понадобятся `opencv-python`.
    ```
    pip install opencv-python
    ```
    И `marimo` для для просмотра интерактивной лабораторной работы
    ```
    pip install marimo
    ```
    _но вы должны были ее установить уже до открытия этого приложения_
    """
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ## 2/ Получение видеопотока

    Вы можете использовать три варианта получения видеопотока: **со встроенной или подключенной камере к вашему ноутбуку, из видеофайла или из локальной сети на занятии в аудитории.**

    /// details | Код для случая видеокамеры или видеофайла
    ```python
    import cv2

    # Захватываем видеопоток с веб-камеры
    cap = cv2.VideoCapture(0)
    # или из файла
    # cap = cv2.VideoCapture('video.mp4')
    while True:
        # Читаем кадр с камеры
        ret, frame = cap.read()
        # Отображаем кадр на экране
        cv2.imshow('WebCam', frame)
        # Ожидание 1 миллисекунды на проверку нажатия клавиши 'q'
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

    # Освобождаем ресурсы после завершения работы
    cap.release()
    # Закрываем все окна
    cv2.destroyAllWindows()
    ```
    ///

    Для получения **видеопотока по локальной сети** необходимо создать UDP сокет и подключится к multicast группе. В главном цикле читается байтовый массив по сокету, который преобразуется в jpeg-кадр. Ниже представлен базоый скрипт клиента получения видеопотока:

    /// details | Код клиента
    ```python
    import socket
    import cv2
    import numpy as np

    # Настройки multicast
    MCAST_GRP = '239.1.1.1'
    MCAST_PORT = 5007

    ############### ВМЕСТО cap = cv2.VideoCapture(0) ####################
    #####################################################################
    # Создаем UDP сокет
    sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM, socket.IPPROTO_UDP)
    sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)

    # Присоединяемся к multicast группе
    sock.bind(('', MCAST_PORT))
    mreq = socket.inet_aton(MCAST_GRP) + socket.inet_aton('0.0.0.0')
    sock.setsockopt(socket.IPPROTO_IP, socket.IP_ADD_MEMBERSHIP, mreq)
    #####################################################################

    # Главный цикл
    while True:
    ############### ВМЕСТО ret, frame = cap.read() ######################
    #####################################################################
        # Получаем данные
        data, addr = sock.recvfrom(65536)  # Макс размер UDP пакета

        # Декодируем JPEG в изображение
        nparr = np.frombuffer(data, np.uint8)
        frame = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
    #####################################################################

        if frame is not None:s
            cv2.imshow('Multicast Stream', frame)

        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

    cv2.destroyAllWindows()
    sock.close()
    ```
    ///
    """
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ## 3/ **Треккеры объектов**

    В прошлой лабораторной работе изучался фильтр Калмана, который позволяет прогнозировать положение объекта на основе его прошлых состояний и текущих измерений. Однако, в реальных задачах компьютерного зрения часто нужно не только предсказывать положение, но и отслеживать объекты, которые могут перемещаться, менять форму или перекрывать другие объекты. В этом ЛР мы рассмотрим различные методы отслеживания движущихся объектов, которые расширяют возможности фильтра Калмана, позволяя отслеживать положение объектов в видеопотоке, обнаруживать новые объекты и даже работать с несколькими объектами одновременно. Мы рассмотрим различные алгоритмы трекинга, их особенности и практическое применение.

    _**Основные задачи отслеживания объектов:**_

    1. Разработка алгоритмов, способных справляться с изменениями в освещении, масштабе, и другими факторами, сохраняя точность отслеживания.

    2. Анализ движения объектов для определения их скорости и направления, что имеет важное значение в различных приложениях, таких как видеонаблюдение.

    3. Разработка методов, способных справляться с ситуациями, когда объекты могут перекрываться или временно исчезать из кадра.

    _**Методы отслеживания движущихся объектов:**_

    1. Корреляционные методы. Оценка схожести между объектами на последовательных кадрах на основе корреляции, что позволяет отслеживать объекты на основе их уникальных признаков.

    2. Оптический поток. Измерение изменений положения пикселей между кадрами, что позволяет определить направление и скорость движения объектов.

    3. Фильтры Калмана и Particle Filter. Прогнозирование перемещения объектов на основе предыдущих данных и коррекция прогнозов с учетом текущих измерений.

    4. Методы машинного обучения. Использование алгоритмов машинного обучения, таких как TrackNet или GOTURN, обученных на больших наборах данных для эффективного отслеживания объектов.

    5. Интеграция с детекцией объектов. Совмещение методов отслеживания с методами детекции для повышения устойчивости и точности системы в условиях изменчивой сцены.

    6. Комплексные методы трекинга.

    На 6 пункт стоит отдельно обратить внимание. Он включает в себя алгоритмы комплексных методов трекинга. Некоторые из широко используемых трекеров включают KLT, CSRT, MedianFlow, TLD и MOSSE и т. д. Эти трекеры обладают различными характеристиками и подходами, что позволяет выбирать наилучший трекер в зависимости от конкретных условий сцены.
    """
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ### 3.1/ **Трекер CSRT (Continuous Adaptive Mean Shift with Color Name and Scale Estimation)** 
    Представляет собой алгоритм отслеживания объектов в видеопотоке. Он применяется в случаях, когда требуется высокая точность отслеживания при изменениях в масштабах, освещении и появлении частичных закрытий объекта. CSRT использует комбинацию методов, включая адаптивное среднее смещение с оценкой цвета и масштаба.

    Метод адаптивного среднего смещения позволяет эффективно справляться с изменениями положения объекта в видеопотоке, а оценка цвета и масштаба помогает адаптироваться к изменениям в окружающей среде. Эти характеристики делают CSRT подходящим для сложных сценариев, где объект может менять размер, цвет или быть частично закрытым.

    Метод адаптивного среднего смещения используется в алгоритме отслеживания объектов в видеопотоке, таком как CSRT. Он начинается с инициализации окна вокруг объекта (область ROI), которой нужно отследить. Затем для каждой точки в этом окне вычисляются характеристики, такие как цвет и текстура, для оценки плотности вероятности. Далее вычисляется центр масс этой плотности, который затем используется для смещения окна в направлении, максимизирующем плотность вероятности. Этот процесс повторяется до тех пор, пока центр окна не перестанет смещаться или не будет достигнут критерий останова. Таким образом, метод адаптивного среднего смещения позволяет адаптироваться к изменениям положения объекта и его характеристикам в видеопотоке, обеспечивая эффективное отслеживание объектов при изменениях в масштабе, освещении и появлении частичных закрытий.

    В библиотеке OpenCV конструктор класса, реализующий трекер CSRT, называется `cv2.TrackerCSRT()` или `cv2.TrackerCSRT_create()`. Он предоставляет удобный интерфейс для использования этого трекера в приложениях компьютерного зрения, обеспечивая высокую стабильность и точность отслеживания объектов в видеопотоке.
    """
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ### 3.2/ **Трекер KCF (Kernelized Correlation Filters)** 
    Представляет собой алгоритм отслеживания объектов, использующий корреляцию шаблона в пространстве признаков. Он применяет ядерные методы для повышения точности и эффективности отслеживания.

    Корреляция шаблона в пространстве признаков — это метод анализа изображений, который позволяет сопоставить шаблон (обычно небольшое изображение, например, заданная область ROI) с изображением более крупного размера с целью определения местонахождения шаблона на большем изображении. Пространство признаков здесь означает множество всех возможных признаков изображения, которые могут быть использованы для описания его содержимого (например, цвет, текстура, углы и т. д.).

    Когда мы говорим о корреляции шаблона в пространстве признаков, мы обычно сначала преобразуем как шаблон, так и большее изображение в пространство признаков. Затем мы ищем степень совпадения или корреляции между шаблоном и каждым фрагментом большего изображения в пространстве признаков. Это может быть выполнено различными способами, например, путем вычисления скалярного произведения между векторами признаков шаблона и фрагмента изображения.

    Когда корреляция максимальна, это указывает на то, что шаблон наиболее точно соответствует фрагменту изображения. Поиск максимальной корреляции позволяет определить местоположение шаблона на большем изображении.

    Алгоритм KCF подходит для сценариев, где требуется высокая точность отслеживания объекта при различных условиях, таких как изменения освещения и масштаба. Он может эффективно работать в реальном времени и обладает устойчивостью к различным видам движения объекта.

    В OpenCV функция, реализующая трекер KCF, предоставляется как `cv2.TrackerKCF()` или `cv2.TrackerKCF_create()`. Этот трекер предоставляет возможность быстрого и точного отслеживания объекта в видеопотоке, используя ядерные методы и корреляцию шаблона.
    """
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ### 3.3/ **Трекер MedianFlow**
    Это алгоритм отслеживания объектов в видеопотоке, который обеспечивает хорошую производительность при относительно небольших изменениях в масштабе, освещении и частичных закрытиях объекта. Этот метод основан на использовании медианного значения для оценки перемещения объекта в кадре.

    Медианное значение – это значение в середине упорядоченного набора данных, разделенного на две равные половины, где половина значений находится выше медианы, а другая половина – ниже. В контексте алгоритма MedianFlow медианное значение используется для оценки перемещения объекта на основе предыдущих значений этого перемещения.

    В алгоритме MedianFlow медианное значение используется для оценки перемещения объекта в кадре. Начиная с инициализации отслеживаемого объекта на первом кадре, алгоритм оценивает его новое положение, опираясь на предыдущее смещение. Медианное значение предыдущих смещений объекта помогает предсказать его ожидаемое перемещение на текущем кадре. Эта оценка может быть скорректирована, учитывая новую информацию о положении объекта, после чего информация о его положении обновляется для использования на следующем кадре. Такой подход позволяет алгоритму адаптироваться к изменениям положения объекта, уменьшая влияние шума или выбросов и повышая точность отслеживания.

    MedianFlow подходит для сценариев, где объект движется плавно, и его форма относительно стабильна. Он может быть эффективен в приложениях, где не требуется высокая точность отслеживания, но важна надежность и скорость работы.

    В OpenCV трекер MedianFlow представлен функцией `cv2.TrackerMedianFlow()` или `cv2.TrackerMedianFlow_create()`. Эта функция предоставляет простой интерфейс для интеграции трекера MedianFlow в приложения на основе компьютерного зрения. Методы, используемые этим трекером, позволяют справляться с небольшими изменениями в положении и форме объекта, что делает его подходящим для определённых видов задач отслеживания объектов в видеопотоке.
    """
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ### 3.4/ **Трекер MOSSE (Minimum Output Sum of Squared Error)**
    Это алгоритм отслеживания объектов, который работает на основе корреляции шаблона. Он использует адаптивные фильтры для обновления шаблона объекта и вычисляет корреляцию между шаблоном и текущим кадром изображения для определения положения объекта.

    Принцип адаптивных фильтров, используемых в алгоритме MOSSE, заключается в обновлении шаблона объекта с учетом изменений в сцене. На первом кадре изображения выбирается шаблон объекта, который представляет собой небольшой фрагмент изображения (например, ROI), содержащий объект, который нужно отследить. Затем адаптивные фильтры используются для обновления шаблона объекта на каждом новом кадре с учетом изменения в положении и внешнем виде объекта в последующих кадрах. Это позволяет алгоритму сохранять актуальный шаблон объекта даже при изменяющихся условиях сцены. После обновления шаблона объекта алгоритм вычисляет корреляцию между обновленным шаблоном и текущим кадром изображения. Корреляция позволяет выявить степень схожести между шаблоном и частью изображения на текущем кадре, что помогает определить положение объекта.

    MOSSE подходит для сценариев, где требуется высокая скорость отслеживания объекта, но не обязательно высокая точность. Он может эффективно работать в реальном времени и подходит для приложений, где объект движется достаточно плавно, и форма объекта относительно стабильна.

    В OpenCV, MOSSE трекер представлен как `cv2.TrackerMOSSE()` или `cv2.TrackerMOSSE_create()`. Этот трекер обеспечивает простой интерфейс для быстрого и адаптивного отслеживания объекта в видеопотоке, что делает его подходящим для определённых сценариев, где важна скорость отслеживания при относительно низких требованиях к точности.
    """
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ### 3.5/ **Трекер MIL (Multiple Instance Learning)**
    Представляет собой алгоритм отслеживания объектов, основанный на методе множественного обучения. В контексте отслеживания объектов, MIL рассматривает объект как совокупность «сумок» (instances) – подобных областей изображения. Обучение происходит на уровне этих сумок, что позволяет алгоритму учитывать изменения внутри объекта. В каждой «сумке» содержится набор объектов, и каждая сумка имеет метку класса (положительная или отрицательная). Целью алгоритма MIL является нахождение гипотезы, которая может классифицировать сумки в соответствии с метками класса, присвоенными им.

    Трекер MIL подходит для сценариев, где объект может менять свою форму или частично перекрываться другими объектами. Он обеспечивает некоторую степень инвариантности к форме объекта, что делает его полезным в условиях переменной окружения.

    В OpenCV функция, реализующая трекер MIL, предоставляется как `cv2.TrackerMIL()` или `cv2.TrackerMIL_create()`. Этот трекер предоставляет средство для обучения на основе множественного обучения и отслеживания объектов в видеопотоке.
    """
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    /// admonition | Примечание
    Если при запуске программы с применением трекеров появляется ошибка отсутствия модуля трекера, то следует установить дополнительные модули в библиотеку OpenCV: 
    ```
    pip install opencv-contrib-python
    ```
    ///
    """
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ## 4/ **Пример реализации отслеживания объекта**

    Рассмотрим реализацию отслеживания объекта на примере. Напомним, что в прошлых лабораторных работах мы произвели подготовку изображения (фильтрация, преобразования цветовых пространств), а также мы позаботились о выделении области интереса ROI. Теперь применим к этой заготовке метод отслеживания. В качестве примера выбран метод трекинга **KCF**.
    """
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ```python
    import cv2

    # Инициализация трекера KCF
    tracker = cv2.TrackerKCF_create()
    cap = cv2.VideoCapture(0)
    # Чтение первого кадра
    ret, frame = cap.read()

    # Определение начальных координат и размеров прямоугольника
    roi = cv2.selectROI(frame, fromCenter=False) # roi=x, y, w, h
    # Инициализация трекера с текущим кадром и прямоугольником
    tracker.init(frame, roi)

    while True:
        ret, frame = cap.read()
        # Обновление трекера и получение новых координат
        success, roi = tracker.update(frame)
        (x, y, w, h) = roi
        # Отрисовка прямоугольника на кадре
        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)
        cv2.imshow('Otselevan', frame)

        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

    # Освобождение ресурсов
    cap.release()
    cv2.destroyAllWindows()
    ```

    Здесь пользователь выбрал прямоугольную область на первом кадре для отслеживания объекта:

    ```python
    roi = cv2.selectROI(frame, fromCenter=False).
    ```

    Трекер инициализируется с текущим кадром и выбранной областью: `tracker.init(frame, roi)`. Это стандартная процедура инициализации трекера в OpenCV.

    Далее в бесконечном цикле происходит захват новых кадров:

    ```python
    ret, frame = cap.read().
    ```

    Трекер обновляет координаты объекта в каждом новом кадре:

    ```python
    success, roi = tracker.update(frame).
    ```

    Полученные координаты объекта используются для отрисовки прямоугольника вокруг объекта:

    ```python
    cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2).
    ```

    Отображается текущий кадр с нарисованным прямоугольником:

    ```python
    cv2.imshow('Otselevan', frame).
    ```

    Уточнение: `tracker.update(frame)` — это вызов метода для обновления состояния трекера с использованием нового кадра из видеопотока. Этот метод принимает текущий кадр и возвращает два значения:

    - `success` (успех) – логическое значение, которое указывает, было ли успешное обновление трекера. Если объект был успешно обнаружен и отслежен на новом кадре, `success` будет `True`. В противном случае, если трекер не смог найти объект, `success` будет `False`.
    - `roi` - координаты региона интереса
    """
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(r"""Заметим, что сменить алгоритм трекинга в данной программе достаточно заменить функцию `cv2.TrackerKCF_create()`, например, на `cv2.TrackerCSRT_create()`. Это удобно для анализа и выбора алгоритма отслеживания. Однако некоторые трекеры могут отсутствовать в той или иной версии библиотеки OpenCV. В этом случае необходимо обратиться к документации библиотеки.""")
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    Если на экране появляется более одного объекта и задача – их отслеживать, применяют мульти-трекер. В библиотеке OpenCV предоставляет удобный инструмент для одновременного отслеживания нескольких объектов в видеопотоке `cv2.MultiTracker()` или `cv2.MultiTracker_create()`.

    В видеопотоке может быть несколько объектов, которые вы хотите отслеживать одновременно, например, несколько автомобилей на дороге или несколько лиц в толпе. Мульти-трекер обеспечивает возможность надежного отслеживания каждого из этих объектов, даже если они частично перекрывают друг друга.

    Еще одним преимуществом мульти-трекера является его гибкость: вы можете легко добавлять или удалять объекты из отслеживания по мере необходимости. Это особенно полезно, когда объекты входят или покидают кадр.

    Кроме того, мульти-трекер позволяет применять различные алгоритмы трекинга к разным объектам. Например, вы можете использовать разные методы для автомобилей и для лиц, что обеспечивает более точное и эффективное отслеживание в различных сценах.

    Таким образом, мульти-трекер является удобным инструментом для обработки сложных сценариев, где требуется отслеживание нескольких объектов в видеопотоке.
    """
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    /// admonition | ✏️ **ЗАДАНИЕ 1.**
    Провести исследование по сравнительному анализу не менее 3 любых трекров. Результаты внести в сравнительную таюлицу ниже.

    | | Трекер 1 | Трекер 2 | Трекер 3 |
    | :--- | :--- | :--- | :--- |
    | Стабильность удержания объекта при его перемещении | | | |
    | Стабильность удержания объекта при его вращении и переворотах | | | |
    | Стабильность удержания объекта при тряске с частотой 1 Гц | | | |
    | Стабильность удержания объекта при тряске с частотой 3-4 Гц | | | |
    | Стабильность удержания объекта при кратковременной потере объекта (0,5 с) | | | |
    | Стабильность удержания объекта при кратковременной потере объекта 1 с | | | |
    | Стабильность удержания объекта при сильном затемнении объекта | | | |
    | Стабильность удержания объекта при частичном перекрытии объекта | | | |

    Каждому трекеру присвойте балл стабильности от **0 до 10** в соответствии с вашим субъективным мнением. В конце напишите **вывод**.
    ///
    """
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ## Форма отчета

    Отчет должен представлять собой программный код для всех заданий и скриншоты экрана, демонстрирующие работоспособоность вашего кода. Отчет загрузить на **moodle**
    """
    )
    return


if __name__ == "__main__":
    app.run()
